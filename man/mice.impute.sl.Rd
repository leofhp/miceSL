% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mice.impute.sl.R
\name{mice.impute.sl}
\alias{mice.impute.sl}
\title{Super learning methods for \code{mice}.}
\usage{
mice.impute.sl(
  y,
  ry,
  x,
  type,
  outcome_type = NULL,
  continuous_learners = NULL,
  binary_learners = NULL,
  categorical_learners = NULL,
  strategy = c("localkernel", "matching", "dirichlet"),
  bootstrap = FALSE,
  sl_engine = c("SuperLearner", "sl3"),
  cv_folds = 10,
  kernel = c("gaussian", "uniform", "triangular"),
  bandwidth = 1,
  alpha = 1,
  ...
)
}
\arguments{
\item{y}{Vector to be imputed.}

\item{ry}{Logical vector of length \code{length(y)} indicating the subset \code{y[ry]} of elements in \code{y} to which the imputation model is fitted. The \code{ry} generally distinguishes the observed (\code{TRUE}) and missing values (\code{FALSE}) in \code{y}.}

\item{x}{Numeric design matrix with \code{length(y)} rows with predictors for \code{y}. Matrix \code{x} may have no missing values.}

\item{type}{Integer vector of length \code{length(y)}, as expected by \code{mice}.}

\item{outcome_type}{Variable scale of \code{y}. One of \code{c("continuous", "binary", "categorical")} or NULL. In the latter case, \code{y} is assumed to be binary if it is a factor with 2 levels, categorical if is a factor with more than 2 levels, and continuous otherwise.}

\item{continuous_learners}{Learning algorithms for continuous variables. Any learning algorithms available in the \code{SuperLearner} or \code{sl3} packages may be specified, depending on \code{sl_engine}. Defaults to mean and glm.}

\item{binary_learners}{Learning algorithms for binary variables. Any learning algorithms available in the \code{SuperLearner} or \code{sl3} packages may be specified, depending on \code{sl_engine}. Defaults to mean and glm.}

\item{categorical_learners}{Learning algorithms for categorical variables. Any learning algorithms available in the \code{sl3} packages may be specified. Defaults to \code{c("Lrnr_mean", "Lrnr_glmnet")}.}

\item{strategy}{Imputation strategy for continuous variables. One of \code{c("localkernel", "matching", "dirichlet")}. Defaults to "localkernel", which corresponds to a local imputation strategy with kernel-based variance estimation, introduced as superMICE by Laqueur et al. 2022.
The "matching" strategy applies predictive mean matching to the super learner predictions, introduced as MISL by Carpenito and Manjourides 2022. The "dirichlet" strategy re-weights the super learner ensemble by sampling from a Dirichlet distribution in each iteration, which we term MIDE (Multiple Imputation by Dirichlet-weighted super learning Ensembles).}

\item{bootstrap}{Logical value indicating whether to train the super learner on a bootstrap sample or the full data in each iteration. Bootstrapping approximates a proper imputation procedure. Defaults to FALSE.}

\item{sl_engine}{Super learner backend for model training. One of \code{c("SuperLearner", "sl3")}. Must be \code{"sl3"} for categorical \code{outcome_type}. Defaults to \code{"SuperLearner"}.}

\item{cv_folds}{Number of cross-validation folds to use in super learner training. Defaults to 10.}

\item{kernel}{If \code{strategy = "localkernel"}, a string specifying the kernel to use in local variance estimation. One of \code{c("gaussian", "uniform", "triangular")}. Defaults to "gaussian".}

\item{bandwidth}{If \code{strategy = "localkernel"}, a positive numeric value or vector specifying the kernel bandwidth. If a vector of candidate bandwidths is supplied, the jackknife selection procedure of Laqueur et al., 2022 is called. Defaults to 1.}

\item{alpha}{If \code{strategy = "dirichlet"}, a positive numeric value for the scaling parameter used in sampling weights from the Dirichlet distribution. Defaults to 1.}

\item{...}{Additional arguments passed to \code{SuperLearner}, such as learner-specific tuning parameters. Ignored if \code{sl_engine = "sl3"}.}
}
\value{
An imputed vector of \code{y} values (as expected by \code{mice}).
}
\description{
Unified super learning method for multiple imputation by chained equations (MICE), which includes the local imputation approach of Laqueur et al., 2022 (superMICE): \code{strategy = "localkernel"}, \code{bootstrap = FALSE} and the matching approach of Carpenito & Manjourides, 2022 (MISL): \code{strategy = "matching"}, \code{bootstrap = TRUE} as special cases.
}
\examples{
# Bivariate data (x, y) with missing values in x based on an MAR mechanism.
set.seed(42)
N <- 50
p <- 0.2
x <- rnorm(n = N, mean = 3, sd = 1)
y <- 10 + 2 * sin(x^2) - 0.05 * exp(x) - 0.5 * x +
  rnorm(n = N, mean = 0, sd = 0.5)
x[(runif(N) < (2 * p * pnorm(y, mean = mean(y), sd = sd(y))))] <- NA
df <- data.frame(x, y)

imps <- mice::mice(
  df, m = 5, maxit = 1, method = "sl", strategy = "localkernel",
  bootstrap = FALSE, bandwidth = 0.5, cv_folds = 5,
  continuous_learners = c("SL.mean", "SL.glm")
)

}
\references{
Laqueur HS, Shev AB, Kagawa RMC. SuperMICE: An Ensemble Machine Learning Approach to Multiple Imputation by Chained Equations. American Journal of Epidemiology. 2022;191(3):516-525.

Carpenito T, Manjourides J. MISL: Multiple imputation by super learning. Statistical Methods in Medical Research. 2022;31(10):1904-1915.
}
